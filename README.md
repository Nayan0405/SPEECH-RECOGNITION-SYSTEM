# SPEECH-RECOGNITION-SYSTEM

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: Nayan G

*INTERN ID*: CT04DH2457

*DOMAIN*: ARTIFICIAL INTELLIGENCE

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTHOSH

DESCRIPTION OF THE PROJECT:
In this second project, instead of using pre-trained transformer models like GPT-2, we explore how to build a text generation model from scratch using LSTM (Long Short-Term Memory) networks. LSTM is a type of Recurrent Neural Network (RNN) that is very effective in learning sequences, especially text. This project uses a combination of Keras (a high-level neural networks API), TensorFlow (as the backend), and Python libraries like NumPy, Matplotlib, and NLTK to build the model.
This project helped me understand how deep learning can be applied at a lower level compared to pre-trained models like GPT-2. I also got a chance to see how models learn patterns from raw text and generate outputs based on character or word-level training.
Keras is the high-level API we used to design and train our LSTM model, while TensorFlow is the backend that performs the actual computations. Keras makes it easier to create complex neural networks with just a few lines of code.

In our project, we defined a Sequential model with one or more LSTM layers followed by Dense (fully connected) layers. We used the .fit() function to train the model on the dataset and then tested its ability to generate new text from learned patterns.

Real-world uses of TensorFlow/Keras:
Deep learning in image and text processing
Recommendation systems like Netflix or YouTube suggestions
Voice recognition and translation systems
Predictive text input in smartphones or search engines
Medical diagnosis tools based on patterns in data

LSTM is a special kind of RNN that can remember long-term dependencies, making it perfect for language generation tasks. Unlike regular RNNs that suffer from the vanishing gradient problem, LSTMs use gates (input, output, forget) to control the flow of information, allowing them to remember useful information over longer sequences.

In our project, LSTM helped the model learn sentence structures, spelling, and grammar over time. It was trained on a text corpus (like books, news articles, or song lyrics), and eventually learned to generate text that looks like it was written by a human.

Real-world applications of LSTM:
Text prediction and auto-complete
Speech recognition systems
Stock price forecasting
Anomaly detection in time series data
Chatbots and virtual assistants

#OUTPUT:
